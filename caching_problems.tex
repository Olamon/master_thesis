\section{Caching problems}
\label{caching_problems}
In this section we make an overview of caching problems. The basic paging 
problem setting is well studied from the competitive analysis perspective. 
Both deterministic and randomized optimal solutions are already known. We focus 
on deterministic algorithms, although we give a sketch of randomized solutions 
because of their popularity and importance in the online algorithm theory as a 
whole. 
\subsection{Basic definitions}
We shall remind ourselves about basic concepts of online algorithms theory. 
Let $\sigma = \sigma(1), \sigma(2), \ldots, \sigma(n)$ be a sequence of 
requests. An \textit{online algorithm} $A$ has to process requests one by one, 
without any knowledge of the future requests. Formally, when 
serving request 
$\sigma(t), 1 \leq t \leq n,$ the algorithm does not know any request 
$\sigma(t')$ for $t'>t$. Handling any request incurs $cost$ whose value is 
depends on request and algorithm state. The goal is to minimize the overall 
$cost$.

Publication written by Sleator and Tarjan \cite{tarjan} proposes 
\textit{competitive analysis} as a way to evaluate performance of online 
algorithms. In this approach we compare \csigma{A}, cost of online algorithm 
$A$ on input $\sigma$, with \csigma{OPT} of optimal offline algorithm, 
which knows all the input sequence beforehand. We call algorithm 
\textit{r-competitive} if there exists constant $c$, such that
$$C_{\mathrm{A}}(\sigma) \leq r \times C_{\mathrm{OPT}}(\sigma) + c$$
for any request sequence $\sigma$. We say that $r$ a \textit{competitive ratio}
of an algorithm if it is a minimal value, such that the algorithm is 
\textit{r-competitive}. The goal is therefore to minimize $r$.

\subsection{Caching and model variants}
Caching problem can be described as follows. We are given two types of memory: 
fast memory which has size $k$ and slow memory (secondary storage) with size $N 
\gg k$. We can 
assume that number of request $n$ fulfills $N \geq n \gg k$. When getting a 
request $\sigma_{e}$ to an item $e$ that is already in cache we serve it 
without any cost (or at the cost $c \ll 1$). 
Otherwise, we have to retrieve $e$ from the slower memory and pay 
$1$. Sometimes we require, whenever $e$ is not in the cache, before 
serving $\sigma_{e}$ we have to place $e$ in cache, therefore to have enough 
free space in fast memory to move it there. If there is no such requirement 
(and $\sigma_{e}$ can 'omit' moving into cache) we say that model is 
\textit{with bypassing}. Usually omitting cache is very expensive. After serving 
request the algorithm can change its cache by evicting and fetching elements 
from secondary storage. Some cost is assigned to these operations, too.

In tree caching problem, that we study, requested element $v$ can depend 
on some other elements. Specifically, they depend on its descendant in a 
way, that when we decide to fetch $v$ we have to fetch all nodes from $T(v)$ 
that are not already in cache.

There is another generalization of caching problem that we mostly focus on in 
this thesis. It is called 
$(k_{\mathrm{ONL}}, k_{\mathrm{OPT}})$\textit{-paging generalization}. It gives 
online and optimal offline algorithm different resources: online cache has size 
$\kind{ONL}$, while optimal offline cache has size 
$k_{\mathrm{OPT}} \leq k_{\mathrm{ONL}}$. It might seem 
unfair to give the optimal solution less space, but sometimes it can give us 
some information how resources can influence the performance. (Remember 
that offline algorithm has much more power: it knows all the future).

\subsection{Deterministic algorithms}
We remind well-known algorithms that solve caching problem without bypassing, 
so every requested item has to be placed in cache and any reference to 
slow memory has unit cost 1.
\subsubsection{Longest Forward Distance (LFD) - offline optimal solution}
Consider following \textit{lazy} (evicts a page if and only if it has no space  
offline algorithm): 
\begin{myalgo}
Whenever there is a cache miss (item requested is not 
present in fast memory), evict element whose next request is latest among other 
cached elements.
\end{myalgo}

This algorithm is called \textbf{Longest Distance Forward} (LDF) and it turns 
out it is the offline optimal solution. That is really uncommon to know the 
optimal offline solution which really helps in competitive analysis giving us 
a lot of knowledge about the problem. Moreover, this algorithm has polynomial 
time complexity, whereas most online theory problem are not solvable in 
polynomial time. The optimality of \textbf{LFD} was proven by Belady 
\cite{LFDBelady}.
\begin{theorem} 
 \textbf{LFD} is optimal offline algorithm for paging.
\end{theorem}
\begin{proof}
 The proof is based on applying following lemma (its proof can be found in 
\cite{czarodziej}).
\begin{lemma}
Let $A$ be any offline algorithm for paging, $\sigma$ - request sequence. For 
any $i \in \{1 \ldots |\sigma|\}$ we can construct offline algorithm $A_i$ such 
that:
\begin{itemize}
 \item[(a)] $A_i$ serves first $i-1$ items the same as $A$,
 \item[(b)] If $i$th request results in page fault, $A_i$ evicts from cache 
item from items in cache which will be latest requested (with 'longest distance 
forward'),
  \item[(c)] $A_i(\sigma) \leq A(\sigma)$.
\end{itemize}
\end{lemma}
Having this lemma, we can prove the theorem by applying it $|\sigma|$ times. 
We start with any $OPT$ algorithm and apply the lemma, so we get $OPT_1$. Then 
we apply lemma to $OPT_1$ for $i=2$ getting $OPT_2$ and so on. It is easy to 
see that $OPT_{|\sigma|}$ acts like \textbf{LFD} and pays cost at most the 
same 
as $OPT$.
\end{proof}

\subsubsection{Marking algorithms examples (LRU, FWF)}
Now we will consider two deterministic algorithms.
\begin{myalgo}
 \textbf{Least Recently Used (LRU)}
 \newline
When eviction is necessary, evict item whose most recent request was the 
earliest among cached items.
\end{myalgo}
\begin{myalgo}
  \textbf{Flush When Full (FWF)}
  \newline
Whenever there is page fault and cache is full evict all elements from cache.
\end{myalgo}
Both of them belong to a class of algorithms which are called \textit{marking}. 
It is characterized by associating a bit to each page and we call 
it its mark. Initially all pages are unmarked. 

We divide input sequence $\sigma$ into $k$\textit{-phases} dependent on value $k$ 
(cache size). We define $i$th phase as a maximal input sequence, following 
phase $i-1$ such that it contains at most $k$ different items requested. So, 
except of the last phase, each phase ends when the next request $r$ would 
increase the number of distinct pages in phase to $k+1$. Useful observation is 
that $r$ has to be \textit{new}, meaning it is different then any item requested 
in phase $i$.
\input{phase_deterministic_pic}

In any marking algorithm, during a phase, we mark a page when it is first 
requested in the phase. Finally, marking algorithm that 
never evicts marked page during a phase. Now we will show theorem 
characterizing this class of marking algorithms.
\begin{theorem}\cite{torng}
For any marking algorithm $A_{\mathrm{mark}}$ with cache of size 
$\kind{ONL}$, if $OPT$ 
cache size is $\kind{OPT} \leq \kind{ONL}$, $A_{\mathrm{mark}}$ is 
$O(\frac{\kind{OPT}}{\kind{ONL}-\kind{OPT}+1})$-competitive.
\end{theorem}
\begin{proof}
Fix any input sequence $\sigma$ and divide it into $\kind{ONL}$-phases as described 
above. $A_{\mathrm{mark}}$ on any such phase pays at most $\kind{ONL}$ (because there is 
at most 
$\kind{ONL}$ distinct requests in one phase). To bound the $OPT$ cost we will bound it with 
respect to \textit{shifted phases}. Shifted phase $i$ consists 
of first element of phase $i+1$ (call it $f_{i+1}$) and all requests from phase 
$i$th except the first one (call it $f_i$). Figure 
~\ref{fig:PhaseDeterministic} shows phases and shifted phases for $\kind{ONL}=3$. See 
that to obtain shifted phase window we moved phase window by one element. At 
the beginning of $i$ shifted phase $OPT$ has to have $f_i$ in it's cache. 
Therefore it can have at most $\kind{OPT}-1$ distinct elements from shifted phase $i$. 
Shifted phase has at least $\kind{ONL}$ distinct elements therefore $OPT$ has at least 
$\kind{ONL}-\kind{OPT}+1$ page faults during shifted phase (there can be at most two shifted 
phases that do not have this property, we omit them, cause the cost can be put 
into the constant when comparing costs).
Summing up costs for $A_{\mathrm{mark}}$ by phases and $OPT$ by shifted phases 
we get:
$$ A_{\mathrm{mark}}(\sigma) \leq \frac{\kind{ONL}}{\kind{ONL}-\kind{OPT}+1} \cdot OPT(\sigma) + const,$$
where we put all the cost obtained at requests not covered by phases (or 
shifted phases).
\end{proof}
Now we should prove that both \textbf{FWF} and \textbf{LRU} are
marking algorithms.
\begin{propo}
 \textbf{FWF} and \textbf{LRU} are marking algorithms.
\end{propo}
\begin{proof}
 We fix request sequence $\sigma$ and consider its $\kind{ONL}$-phase division 
(where $\kind{ONL}$ is cache size of any of the two online algorithms).

First, see that \textbf{FWF} is marking algorithm. At the start of each phase 
its cache is empty and it does not evict any pages during whole phase, 
therefore it certainly does not evict any marked page.

Now suppose that \textbf{LRU} is not marking algorithm. Then there exists a 
phase $p$ and element $x$ that is marked during this phase and evicted before 
the phase ends. Assume that both $p$ and $x$ are first such phase and element. 
Consider the moment when $x$ is marked (first time it was requested in $p$). 
For that time it is the most recently requested element. For the \textbf{LRU} to 
evict $x$, it has to be 'the oldest' element in cache, therefore there must be 
$\kind{ONL}$ elements distinct from $x$ requested. The eviction takes place 
during $p$ so, it must contain 
at least $\kind{ONL}+1$ distinct elements which contradicts with $\kind{ONL}$-phase definition. 
\end{proof}
This gives us that both algorithms are $O(\frac{\kind{ONL}}{\kind{ONL}-\kind{OPT}+1})$-competitive. Not 
only marking algorithms obtain this ratio. One example of such algorithm is 
\textbf{First-in First-out} (FIFO), which evicts a page that was in cache for 
the longest time.

\subsubsection{Non competitive algorithms examples}
In this part we consider two well known strategies that turn out to be not 
competitive at all.
\begin{myalgo}
 \textbf{Last-in First-out (LIFO)}
 \newline
If removal is necessary, evict the element which was moved to cache most 
recently.
\end{myalgo}
\begin{myalgo}
 \textbf{Least Frequently Used (LFU)}
 \newline
Remove the element that which was least frequently requested since start of its 
presence in cache.
\end{myalgo}

To prove that those algorithms are not competitive, we have to show a sequence 
of requests that potentially occur infinite cost when compared to cost of 
\textbf{OPT}, which is finite. In cases of both algorithms presented above we 
will use only 
$k+1$ different pages, denoted $p_1, p_2, \ldots, p_{k+1}$ where $k$ is the 
size of cache.

We start with \textbf{LIFO}. Suppose that at the beginning \textbf{LIFO} holds
elements $p_1, p_2, \ldots p_k$ in its cache (we can always renumber the 
elements, so we can 
assume that). Consider the following sequence of requests:
$$ \sigma = (p_1, p_2, \ldots, p_k, p_{k+1}, p_k, p_{k+1}, \ldots).$$
The idea is we request the elements that \textbf{LIFO} just removed from the 
cache. See that on every request after first $p_{k+1}$ this algorithm pays 1, 
so the total cost tends to infinity, whereas \textbf{OPT}, keeping $p_k$ 
and $p_{k+1}$ in its cache gets only one page fault on this subsequence. 
This proves the non-competitiveness of \textbf{LIFO}.

In the case of \textbf{LFU}, fix any constant integer $c$ and sequence:
$$\sigma = (p_1^c, p_2^c, \ldots, p_{k-1}^c, (p_k, p_{k+1})^{c-1}),$$
where superscript means how many times we repeat the request. We can see that 
after first $(k-1) \cdot c + 1$ requests \textbf{LFU} will pay for each of the 
following requests, so it pays $O(c)$ for all the sequence. On the other hand 
\textbf{OPT} will pay only for one page fault when $p_{k+1}$ occurs for the 
first time. Since we can fix $c$ to be any value and we can repeat this 
sequence many times, we can get infinite sequence for which 
$\frac{cost(LFU)}{cost(OPT)} = c \rightarrow \infty$. 
\textbf{LFU} is then not competitive.
\subsection{Randomized algorithms (RAND, MARK)}
When analyzing deterministic algorithms we had only one model of 
\textit{adversary}. His task was to figure out a sequence of requests to 
maximize the competitive ratio of online algorithms. Looking at randomized 
algorithms the adversary can be given less or more power. Does he have to 
provide all the sequence explicitly or can it generate it in on-line fashion? 
Does he knows the algorithm and randomization used by it? Having this questions 
in mind we define different adversaries when dealing with randomized 
algorithms. Usually we recognize three types of adversaries
(see also ~\ref{bbktw}):
\begin{itemize}
\item \textbf{Oblivious Adversary:} The adversary has to show all the request 
sequence before any of them is processed my online algorithm, which is known to 
adversary. The adversary cost is the cost of optimal offline algorithm on that 
sequence. Notice that this adversary is similar to the one used when analyzing
deterministic algorithms.
\item \textbf{Adaptive Online Adversary:} This one can observe the online 
algorithm and generate next request using knowledge about previous (randomized) 
answers of online algorithm. It has to serve requests online though, without 
knowing online algorithm action on present and future requests.
\item \textbf{Adaptive Offline Adversary:} Same as above with the difference in 
way of charging adversary: it obtains cost equal to optimal offline algorithm 
on generated sequence.
\end{itemize}
The competitive ratio for randomized algorithm is dependent on the type of 
adversary we choose. The easiest definition is for oblivious adversary. We say 
that online algorithm $A$ id $c$-competitive against oblivious adversary if for 
all sequences $\sigma$ generated by this adversary, we have:
$$\mathbb{E}[cost_{\mathrm{A}}(\sigma)] \leq c\cdot cost_{\mathrm{OPT}}(\sigma) 
+ const,$$
where the expected value is taken over the random choices of $A$. For adaptive 
adversaries on the other hand we say $A$ is $c$-competitive if:
$$\mathbb{E}[cost_{\mathrm{A}}(\sigma) - c \cdot cost_{\mathrm{ADV}}(\sigma)]
\leq const.$$
Here the expected value is still taken over the choices of $A$. The difference 
is that the $cost_{\mathrm{ADV}}$ is random variable dependent on randomization 
of $A$. 
In case of adaptive offline adversary at least we know that 
$cost_{\mathrm{ADV}}(\sigma)$ and $cost_{\mathrm{OPT}}(\sigma)$ has the same 
distribution, whereas for online 
adaptive adversary so simple nature of its cost random variable is not known.

It turns out that randomization can't give an improvement on competitive 
ration in the case of adaptive offline adversary. The following theorem was 
presented in \cite{power}:
\begin{theorem}
If there is a $c$-competitive algorithm against adaptive offline adversary then 
there exists a $c$-competitive deterministic online algorithm. 
\end{theorem}
The relation between adversaries is captured in the same paper and presented in 
theorem above.
\begin{theorem}
Suppose $A$ is $c$-competitive randomized algorithm against any adaptive online 
adversary and additionally there exists $d$-competitive algorithm against any 
oblivious adversary then $A$ is $c \cdot d$-competitive against adaptive 
offline adversary.
\end{theorem}
Usually we say the oblivious adversary 'weak', adaptive online 'medium' and 
adaptive offline: 'strong'. This informal names are consistent with intuition: 
oblivious adversary is the easiest to compete with, whereas adaptive offline is 
the hardest being the most powerful one.

Now we present a randomized algorithm \textbf{MARK} which turns out to be 
$2 \cdot H_k$-competitive against oblivious adversary. $H_k$ is the called $k$
\textit{th harmonic number} and is defined as $H_k = \sum^k_{i=1} \frac{1}{i}$. 
This series has a really nice property that $\ln k < H_k \leq \ln k + 1$. 
Algorithm \textbf{MARK} is a marking algorithm so it has a bit associated with 
each of $k$ entries in cache.

\begin{algorithm}
\caption{\textbf{MARK}}
\label{alg:MARK}
\begin{algorithmic}[1]
\ForAll{$r$ in sequence $\sigma$}
  \If{$r$ is in cache}
    \State Mark $r$.
  \Else
    \If{Cache is full}
      \If{All elements are marked}
	\State Unmark all cached elements.
      \EndIf
      \State Choose $p$ uniformly at random from unmarked pages.
      \State Remove $p$ from cache.
    \EndIf
    \State Fetch $r$ and mark it.
  \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}
\begin{theorem}
\cite{markcom} Algorithm \textbf{MARK} is $2 \cdot H_k$-competitive against oblivious 
adversary.
\end{theorem}
\begin{proof}
We fix input sequence $\sigma$ and its $k$-phases partition defined the same as 
in case of deterministic marking algorithm. We can assume that both 
\textbf{OPT} and \textbf{MARK} start with the same cache state (adjusting the 
cache can be done with constant cost not influencing the competitive ratio).

Consider the $i$th phase. We call \textbf{old pages} all the elements requested 
in 
phase $i$th that were requested in phase $i-1$. All the other pages requested 
in phase $i$ are \textit{new}. For the first phase, all the pages are new. We 
denote the number of new pages for phase $i$ by $m_i$. The picture of above 
definition can be found in Figure \ref{fig:PhasesRandomized}.
\input{phase_randomized_pic}

We can see that old pages are somehow beneficial for \textbf{MARK}. If it keeps 
an old page sufficiently long (up to the moment when it is first time requested 
in phase $i$), it does not pay for this page through whole phase. Seeing that 
leads us to conclusion that the worst case is when all the new 
$m_i$ elements are requested, then the old pages, so the probability of an old 
page being still in cache is smallest possible. More specifically the 
probability that the $jth$ old page (their order is the order of first 
occurrence 
in phase) is still in cache when requested can be bounded from below by 
$\frac{k-m_i-(j-1)}{k-(j-1)}$ since $k-m_i-(j-1)$ is the number of old pages in 
the cache that are unmarked and  $k-(j-1)$ is the number of unmarked old pages 
up to the time $j$th page is requested for the first time. This gives us that 
the probability of page fault on that request is equal to $1 - 
\frac{k-m_i-(j-1)}{k-(j-1)} = \frac{m_i}{k-(j-1)}$, so the expected number of 
page faults in whole phase is (recall that we pay $1$ for every new page):
$$ m_i + \sum_{j=1}^{k - m_i}\frac{m_i}{k-(j-1)} = m_i\cdot(1+H_k-H_{m_i}) \leq 
m_i \cdot H_k.$$

Now we have to lower bound the cost of \textbf{OPT}. Consider two consecutive 
phases $i-1$ and $i$. There are $k + m_i$ distinct pages requested in that two 
phases. Since \textbf{OPT} cache size is $k$, there are at least $m_i$ faults. 
During the first phase \textbf{OPT} has at least $m_1$ faults. Summing up the 
number of faults in 
the pairs of phases $(i-1, i)$ we get $\sum m_i$ page faults. Since each phase 
is counted at most twice 
(except first one, which is counted once), the lower bound for 
\textbf{OPT} cost is $\frac{1}{2} \cdot \sum m_i$. As we know \textbf{MARK's} 
number of page faults, taking sum from every phase, is at most $H_k \cdot \sum 
m_i$ which proves the theorem. 
\end{proof}
