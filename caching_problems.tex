\section{Caching problems}
In this section we make an overview of caching problems. The basic paging 
problem setting is well studied from the competitive analysis perspective. 
Both deterministic and randomized optimal solutions are already known. We focus 
on deterministic algorithms, although we give a sketch of randomized solutions 
because of their popularity and importance in the online algorithm theory as a 
whole. 
\subsection{Basic definitions}
We shell remind ourselves about basic concepts of online algorithms theory. 
Let $\sigma = \sigma(1), \sigma(2), \ldots, \sigma(n)$ be a sequence of 
requests. An $online algorithm A$ has to process requests one by one, $online$, 
without any knowledge of the future requests. Formally, when serving request 
$\sigma(t), 1 \leq t \leq n,$ the algorithm does not know any request 
$\sigma(t')$ for $t'>t$. Handling any request incurs $cost$ whose value is 
depends on request and algorithm state. The goal is to minimize the overall 
$cost$.

Precursory publication written by Sleator and Tarjan proposes 
\textit{competitive analysis} as a way to compare preformance of online 
algorithms. In this approach we compare $C_A(\sigma)$, cost of online algorithm 
$A$ on input $\sigma$, with $C_{OPT}(\sigma)$ of optimal offline algorithm, 
which knows all the input sequence beforehand. We call algorithm $r-competitive$ 
if there exist constant $c$ such that
$$C_A(\sigma) \leq r \times C_{OPT}(\sigma) + c$$
for any request sequence $\sigma$. We call $r$ a \textit{competitive ratio}. 
The goal is therefore to minimize $r$.

\subsection{Caching and model variants}
Caching problem can be described as follows. We are given two types of memory: 
fast memory which has size $k$ and slow memory but its size $N \gg k$. We can 
assume that number of request $n$ fulfills $N \geq n \gg k$. When getting a 
request $\sigma_{it}$to an item $it$ that is already in cache we serve it 
without obtaining no cost (or the cost $c \ll 1$). 
Otherwise we have to retrieve $it$ from the secondary (slower) memory and pay 
$1$ ($1/c$). Sometime we require, whenever $it$ is not in the cache, before 
serving $\sigma_{it}$ we have to place $it$ in cache therefore have enough 
free space in fast memory to move it there. If there is no such requirement 
(and $\sigma_{it}$ can 'omit' moving into cache) we say that model is 
\textit{with bypassing}. Usually omitting cache is very expensive. After serving 
request the algorithm can change its cache by evicting and fetching elements 
from secondary storage. Some cost is assigned to this operations, too.

In tree caching problem, that we study, requested element $v$ can be dependent 
on some other elements. Specifically, they are dependent on its descendant in a 
way, that when we decide to fetch $v$ we have to fetch all nodes from $T(v)$ 
that are not already in cache.

There is another generalization of caching problem that we consider. It is 
called $(k_{ONL}, k_{OPT})$\textit{-paging generalization}. It gives online and 
optimal offline algorithm different resources: online cache has size $k_{ONL}$, 
while optimal offline cache has size $k_{OPT} \leq k_{ONL}$. It might seems 
unfair to give the optimal solution less space, but sometimes it can give us 
some information how resources can influence the performance (and remember that 
offline algorithm has much more power, it knows all the future). 

\subsection{Deterministic algorithms}
We remind well-known algorithms that solve caching problem without bypassing, 
so every requested item has to be placed in cache and any reference to 
secondary memory has unit cost 1.
\subsubsection{Longest Forward Distance (LFD) - offline optimal solution}
Consider following \textit{lazy} (evicts a page if and only if it has no space  
offline algorithm: 
\begin{myalgo}
Whenever there is a cache miss (item requested is not 
present in fast memory) evict element whose next request is latest among other 
cached elements.
\end{myalgo}

This algorithm is called \textbf{Longest Distance Forward} (LDF) and it turns 
out it is the offline optimal solution. That is really uncommon to know the 
optimal offline solution which really helps in competitive analysis giving us 
a lot of knowledge about the problem. More over, this algorithm has polinomial 
time complexity, whereas most online theory problem are NP-hard.The optimality 
of \textbf{LFD} was proven by Belady.
\begin{theorem}
 \textbf{LFD} is optimal offline algorithm for paging.
\end{theorem}
\begin{proof}
 The proof is based on applying following lemma. For its proof look at [?].
\begin{lemma}
Let $A$ be any offline algorithm for paging, $\sigma$ - request sequence. For 
any $i \in \{1 \ldots |\sigma|\}$ we can construct offline algorithm $A_i$ such 
that:
\begin{itemize}
 \item[(a)] $A_i$ serves first $i-1$ items the same as $A$,
 \item[(b)] If $i$th request results in page fault, $A_i$ evicts from cache 
item from items in cache which will be latest requested (with 'longest distance 
forward'),
  \item[(c)] $A_i(\sigma) \leq A(\sigma)$.
\end{itemize}
\end{lemma}
Having this lemma, we can prove the theorem by applying it $|\sigma|$ times. 
We start with any $OPT$ algorithm and apply the lemma, so we get $OPT_1$. Then 
we apply lemma to $OPT_1$ for $i=2$ getting $OPT_2$ and so on. It is easy to 
see that $OPT_{|\sigma|}$ acts like \textbf{LFD} and pays cost at most the 
same 
as $OPT$.
\end{proof}

\subsubsection{Marking algorithms examples (LRU, FWF)}
Now we will consider two deterministic algorithms.
\begin{myalgo}
 \textbf{Least Recently Used (LRU)}
 \newline
When eviction is necessary, evict item whose most recent request was the 
earliest among cached items.
\end{myalgo}
\begin{myalgo}
  \textbf{Flush When Full (FWF)}
  \newline
Whenever the is page fault and cache is full evict all elements from cache.
\end{myalgo}
Both of them belong to a class of algorithms which is called \textit{marking 
algorithms}. It is characterized by associating a bit to each page and we call 
it its mark. Initially all pages are unmarked. 

We divide input sequence $\sigma$ into \textit{k-phases} dependent on value $k$ 
(cache size). We define $i$th phase as a maximal input sequence, following 
phase $i-1$ such that it contains at most k different items requested. So, 
except of the last phase, each phase ends when the next request $r$ would 
increase the number of distinct pages in phase to $k+1$. Useful observation is 
that $r$ has to be \textit{new}, meaning it is different then any item requested 
in phase $i$.
\input{phase_deterministic_pic}

In any marking algorithm, during a phase, we mark a page when it is first 
requested in the phase. Finally, marking algorithm is such an algorithm that 
never evicts marked page. Now we will show theorem characterizing this class of 
algorithms.
\begin{theorem}
For any $A_{mark}$ marking algorithm with cache of size $k$, if $OPT$ cache 
size is $h \leq k$, $A_{mark}$ is $O(\frac{k}{k-h+1})$-competitive.
\end{theorem}
\begin{proof}
Fix any input sequence $\sigma$ and divide it into $k$-phases as described 
above. $A_{mark}$ on any such phase pays at most $k$ (because there is at most 
k distinct requests in one phase). To bound the $OPT$ cost we will bound it with 
respect to \textit{shifted phases}. Shifted phase $i$ consists 
of first element of phase $i+1$ (call it $f_{i+1}$) and all requests from phase 
$i$th except the first one (call it $f_i$). Figure 
~\ref{fig:PhaseDeterministic} shows phases and shifted phases for $k=3$. See 
that to obtain shifted phase window we moved phase window by one element. At 
the beginning of $i$ shifted phase $OPT$ has to have $f_i$ in it's cache. 
Therefore it can have at most $h-1$ distinct elements from shifted phase $i$. 
Shifted phase has at least $k$ distinct elements therefore $OPT$ has at least 
$k-h+1$ page faults during shifted phase (there can be at most two shifted 
phases that do not have this property, we omit them, cause the cost can be put 
into the constant when comparing costs).
Summing up costs for $A_{mark}$ by phases and $OPT$ by shifted phases we get:
$$ A_{mark}(\sigma) \leq \frac{k}{k-h+1} \cdot OPT(\sigma) + const,$$
where we put all the cost obtained at requests not covered by phases (or 
shifted phases).
\end{proof}
Now we should prove that both \textbf{FWF} and \textbf{LRU} are
marking algorithms.
\begin{propo}
 \textbf{FWF} and \textbf{LRU} are marking algorithms.
\end{propo}
\begin{proof}
 We fix request sequence $\sigma$ and consider its $k$-phase division 
(where $k$ is cache size).

First, see that \textbf{FWF} is marking algorithm. At the start of each phase 
it's cache is empty and it does not evict any pages during whole phase, 
therefore it certainly does not evict any marked page.

Now suppose that \textbf{LRU} is not marking algorithm. Then there exists a 
phase $p$ and element $x$ that is marked during this phase and evicted before 
the phase ends. Assume that both $p$ and $x$ are first such phase and element. 
Consider the moment when $x$ is marked (first time it was requested in $p$). 
For that time it is the most recently requesed element. For the \textbf{LRU} to 
evict $x$ there must be $k$ elements distinct from $x$ requested for it to turn 
to be 'the oldest' in cache. The evicion takes place during $p$ so it must have 
at least $k+1$ distinct elements which contradicts with $k$-phase deffinition. 
\end{proof}
This gives us that both algorithms are $O(\frac{k}{k-h+1})$-competitive. Not 
only marking algorithms obtain this ratio. One example of such algorithm is 
\textbf{First-in First-out} (FIFO), which evicts a page that was in cache for 
the longest time.

\subsubsection{Non competitive algorithms examples}

\subsection{Randomized algorithms (RAND, MARK)}
