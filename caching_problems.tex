\section{Caching problems}
In this section we make an overview of caching problems. The basic paging 
problem setting is well studied from the competitive analysis perspective. 
Both determinisic and randomized optimal solutions are already known. We focus 
on deterministic algorithms, although we give a sketch of randomized solutions 
because of their popularity and importance in the online algorithm theory as a 
whole. 
\subsection{Basic definitions}
We shell remind ourselves about basic conceptst of online algorithms theory. 
Let $\sigma = \sigma(1), \sigma(2), \ldots, \sigma(n)$ be a sequence of 
requests. An $online algorithm A$ has to process requests one by one, $online$, 
without any knowledge of the future requests. Formally, when serving request 
$\sigma(t), 1 \leq t \leq n,$ the algotithm does not know any request 
$\sigma(t')$ for $t'>t$. Handling any request incures $cost$ whose value is 
depends on request and algorithm state. The goal is to minimize the overall 
$cost$.

Precursory publication written by Sleator and Tarjan proposes 
\textit{competitive analysis} as a way to compare preformace of online 
algorithms. In this approach we compare $C_A(\sigma)$, cost of online algorithm 
$A$ on input $\sigma$, with $C_{OPT}(\sigma)$ of optimal offline algorithm, 
which knows all the input sequence beforehand. We call algorithm $r-competitive$ 
if there exist constant $c$ such that
$$C_A(\sigma) \leq r \times C_{OPT}(\sigma) + c$$
for any request sequence $\sigma$. We call $r$ a \textit{competitive ratio}. 
The goal is therefore to minimize $r$.

\subsection{Caching and model variants}
Caching problem can be described as follows. We are given two types of memory: 
fast memory which has size $k$ and slow memory but its size $N \gg k$. We can 
assume that number of request $n$ fullfills $N \geq n \gg k$. When getting a 
request $\sigma_{it}$to an item $it$ that is already in cache we serve it 
without obtaining no cost (or the cost $c \ll 1$). 
Otherwise we have to retrieve $it$ from the secondary (slower) memory and pay 
$1$ ($1/c$). Sometime we require, whenever $it$ is not in the cache, before 
serving $\sigma_{it}$ we have to place $it$ in cache therefore have eanough 
free space in fast memory to move it there. If there is no such requirement 
(and $\sigma_{it}$ can 'ommit' moving into cache) we say that model is 
\textit{with bypassing}. Usually ommiting cache is very costfull. After serving 
request the algorithm can change its cache by evicting and fetching elements 
from secondary storage. Some cost is assigned to this operations, too.

In tree caching problem, that we study, requested element $v$ can be dependent 
on some other elements. Specifically, they are dependent on its descendant in a 
way, that when we decide to fetch $v$ we have to fetch all nodes from $T(v)$ 
that are not alrady in cache.

There is another generalization of caching problem that we consider. It is 
called $(k_{ONL}, k_{OPT})$\textit{-paging generalization}. It gives online and 
optimal offline algorithm diffrent resources: online cache has size $k_{ONL}$, 
while optimal offline cahce has size $k_{OPT} \leq k_{ONL}$. It might seems 
unfair to give the optimal solution less space, but sometimes it can give us 
some information how resources can influence the performance (and remember that 
offline algorithm has much more power, it knows all the future). 

\subsection{Deterministic algorithms}
We remind well-known algorithms that solve caching problem without bypassing, 
so every requested item has to be placesd in cache and any reference to 
secondary memory has unit cost 1.
\subsubsection{Longest Forward Distance (LFD) - offline optimal solution}
Consider following \textit{lazy} (evicts a page if and only if it has no space  
offline algorithm: 
\begin{myalgo}
Whenever there is a cache miss (item requested is not 
present in fast memory) evict element whose next request is latest among other 
cached elements.
\end{myalgo}

This algorithm is callled \textbf{Longest Distance Forward} (LDF) and it turnes 
out it is the offline optimal solution. That is really uncommon to know the 
optimal offline solution which really helps in competitive analysis giving us 
a lot of knowledge about the problem. More over, this algorithm has polinomial 
time complexity, whereas most online theory problem are NP-hard.The optimality 
of \textbf{LFD} was proven by Belady.
\begin{theorem}
 \textbf{LFD} is optimal offline algorithm for paging.
\end{theorem}
\begin{proof}
 The proof is based on applying following lemma. For its proof look at [?].
\begin{lemma}
Let $A$ be any offline algorithm for paging, $\sigma$ - request sequence. For 
amy $i \in \{1 \ldots |\sigma|\}$ we can construct offline algorithm $A_i$ such 
that:
\begin{itemize}
 \item[(a)] $A_i$ serves first $i-1$ items the same as $A$,
 \item[(b)] If $i$th request results in page fault, $A_i$ evicts from cache 
item from items in cache which will be latest requested (with 'longest distance 
forward'),
  \item[(c)] $A_i(\sigma) \leq A(\sigma)$.
\end{itemize}
\end{lemma}
Having this lemma, we can prove the theorem by applying it $|\sigma|$ times. 
We start with any $OPT$ algorithm and apply the lemma, so we get $OPT_1$. Then 
we apply lemma to $OPT_1$ for $i=2$ getting $OPT_2$ and so on. It is easy to 
see that $OPT_{|\sigma|}$ acts like \textbf{LFD} and pays cost at most the 
same 
as $OPT$.
\end{proof}

\subsubsection{Marking algorithms examples (LRU, FIFO)}
\subsubsection{Non competitive algortithms examples}

\subsection{Randomized algorithms (RAND, MARK)}